# ğŸ§  Multi-Step Reasoning Agent with Self-Checking

---
Streamlit app link: https://multi-step-reasoning-agent-with-self-checking.streamlit.app/
---

A lightweight multi-step reasoning agent built using a **Planner â†’ Executor â†’ Verifier** architecture with automatic retries, logging, and evaluation.

The system produces:
- A clean, user-facing answer
- A machine-friendly JSON output
- Internal metadata (plan, checks, retries)
- Persistent logs (CSV / JSON) for evaluation

Designed for structured word problems such as arithmetic, time, and multi-step reasoning tasks.

---

## ğŸš€ Key Features

### ğŸ§© Multi-Phase Reasoning Pipeline

1. Planner  
   - Reads the question  
   - Produces a structured step-by-step plan  
   - Output is JSON-friendly and deterministic  

2. Executor  
   - Executes the plan exactly  
   - Performs calculations  
   - Returns intermediate_result and factual step records  
   - No chain-of-thought exposed  

3. Verifier  
   - Independently validates executor output  
   - Recomputes results  
   - Flags incorrect reasoning  
   - Triggers retries if needed  

4. Retry Controller  
   - Automatically retries up to a fixed limit  
   - Tracks retry count for evaluation  

---

## âœ… Expected JSON Output

Example response:
```
{
  "answer": 350,
  "status": "success",
  "reasoning_visible_to_user": "I solved this by planning, executing, and verifying the result.",
  "metadata": {
    "plan": {
      "steps": [
        "Calculate distance at 50 km/hr for 3 hours",
        "Calculate distance at 80 km/hr for 2 hours",
        "Sum both distances"
      ]
    },
    "checks": [
      {
        "check_name": "verification_passed",
        "passed": true,
        "details": ""
      }
    ],
    "retries": 0
  }
}
```
---

## ğŸ“ Project Structure
```
MultiStep_Reasoning/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ graph.py              # LangGraph definition
â”‚   â”œâ”€â”€ nodes.py              # Planner / Executor / Verifier nodes
â”‚   â””â”€â”€ graph_state.py        # State schema
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ planner_prompt.txt
â”‚   â”œâ”€â”€ executor_prompt.txt
â”‚   â””â”€â”€ verifier_prompt.txt
â”‚
â”œâ”€â”€ solve.py                  # Core solve() API
â”œâ”€â”€ app.py                    # Streamlit UI
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ easy_tests.py
â”‚   â”œâ”€â”€ tricky_tests.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py             # CSV / JSON logger
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ run_logs.csv
â”‚   
â”‚
â”œâ”€â”€ run_tests.py              # Test runner
â””â”€â”€ print_summary.py          # Test analytics
```
---

## ğŸ–¥ï¸ How to Run the Application

Start the Streamlit App:

```
streamlit run app.py
```
- Enter a question
- View final answer, explanation, retries, verifier checks, and raw JSON output

---

## ğŸ§ª Evaluation & Test Suite

The project includes a custom test framework to evaluate reasoning quality and robustness.

### Test Categories

Easy Tests:
- Basic arithmetic
- Speed Ã— time Ã— distance
- Simple time differences

Tricky Tests:
- Multi-step reasoning
- Ambiguous phrasing
- Time boundary cases
- Edge conditions

---

## â–¶ï¸ How to Run Tests

Run all tests:

python run_tests.py

Run individually:

```
python -m tests.easy_tests  
python -m tests.tricky_tests  
```
---

## ğŸ§ª What Each Test Logs

For every test case, the following are recorded:

- Question  
- Final JSON output  
- Whether the verifier passed  
- Number of retries performed  

---

## ğŸ“„ Logs Export

All runs (tests + app usage) are automatically logged to:

- logs/run_logs.csv  
 
---

## ğŸ” What the Logs Help Evaluate

- Planner accuracy  
- Executor consistency  
- Verifier effectiveness  
- Retry behavior  

---

## ğŸ“Š Auto-Generated Test Summary

Category | Test Count | Verifier Pass Rate | Retries Observed  
Easy     | 5â€“10       | ~100%              | 0â€“1  
Tricky  | 3â€“5        | ~85â€“90%            | 1â€“2  

---

## ğŸ“ˆ Retry & Quality Metrics

Using print_summary.py, the system computes:
- Retry rate
- Average retries per question
- Planner / Executor quality indicators
- Failure patterns

---
## ğŸ§  Prompt Design & Rationale

This project uses **role-specific prompts** for each agent stage (Planner, Executor, Verifier) to enforce separation of concerns and improve reasoning reliability.

---

### ğŸ”¹ Why the Prompts Were Designed This Way

#### **1. Planner Prompt**
**Goal:** Convert a natural language question into a deterministic, structured plan.

**Design choices:**
- Forces numbered, sequential steps
- Avoids computation or reasoning
- Produces JSON-friendly output

**Why this works:**
- Prevents early arithmetic mistakes
- Makes multi-step problems explicit
- Enables retries by re-planning instead of guessing

**Example responsibilities:**
- Identify inputs (numbers, units, times)
- Decide computation order
- Specify transformations (sum, difference, conversion)

---

#### **2. Executor Prompt**
**Goal:** Execute the plan *exactly as written* and produce a machine-checkable result.

**Design choices:**
- Strict JSON schema (intermediate_result + steps)
- One factual line per step
- No free-form explanation or verification

**Why this works:**
- Keeps execution deterministic
- Makes debugging and testing easy
- Allows verifier to independently recompute results

**Executor never:**
- Fixes planner mistakes
- Verifies correctness
- Adds assumptions silently

---

#### **3. Verifier Prompt**
**Goal:** Independently validate the executorâ€™s result.

**Design choices:**
- Recomputes from scratch
- Outputs pass/fail + short summary
- No chain-of-thought exposure

**Why this works:**
- Catches arithmetic slips
- Detects plan/execution mismatch
- Enables controlled retries without infinite loops

---

### âš ï¸ What Didnâ€™t Work Well Initially

1. **Single-prompt reasoning**
   - Mixing planning, execution, and verification caused hallucinations
   - Difficult to debug or retry

2. **Free-form Executor output**
   - Inconsistent formats broke tests
   - Verifier could not reliably parse results

3. **Verbose chain-of-thought**
   - Caused token overuse
   - Reduced determinism
   - Violated safe-output constraints

4. **Implicit assumptions**
   - Ambiguous inputs (time ranges, units) caused silent errors
   - Fixed by forcing explicit assumptions into metadata

---

### ğŸš€ What I Would Improve With More Time

1. **Prompt caching**
   - Cache Planner outputs for repeated or similar questions
   - Reduce token usage and rate-limit issues

2. **Stronger Verifier heuristics**
   - Add multiple independent checks (units, bounds, sanity checks)
   - Score confidence instead of binary pass/fail

3. **Executor fallback logic**
   - Use pure Python for arithmetic-only plans
   - Call LLM only when interpretation is required

4. **Adaptive retry strategy**
   - Change prompts dynamically after repeated failures
   - Example: simplify plan or reduce step count

5. **Benchmark-driven prompt tuning**
   - Optimize prompts based on test pass rate
   - Track failure modes across easy vs tricky cases

---

### âœ… Outcome

This prompt design achieves:
- **High determinism**
- **Clear auditability**
- **Reliable self-correction**
- **Clean JSON outputs suitable for evaluation**

It balances LLM flexibility with strong structural constraints, making the system suitable for both **interactive use** and **automated testing**.

---

## âš ï¸ Rate-Limit Notes

- Running many tests consumes LLM tokens  
- For testing: use smaller models  
- For demo / production: use gpt-4o-mini  

---

## ğŸš§ Future Improvements

- Local math executor (no LLM for arithmetic)
- Caching planner outputs
- Parallel verifier strategies
- Streaming UI responses
- Confidence scoring

---

## âœ… Summary

- Clean architecture  
- Deterministic execution  
- Self-verification  
- Retry logic  
- Full evaluation framework  
- Production-ready logging  

