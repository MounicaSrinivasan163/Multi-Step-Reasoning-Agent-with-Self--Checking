# ğŸ§  Multi-Step Reasoning Agent with Self-Checking

---
Streamlit app link: https://multi-step-reasoning-agent-with-self-checking.streamlit.app/
---

A lightweight multi-step reasoning agent built using a **Planner â†’ Executor â†’ Verifier** architecture with automatic retries, logging, and evaluation.

The system produces:
- A clean, user-facing answer
- A machine-friendly JSON output
- Internal metadata (plan, checks, retries)
- Persistent logs (CSV / JSON) for evaluation

Designed for structured word problems such as arithmetic, time, and multi-step reasoning tasks.

---

## ğŸš€ Key Features

### ğŸ§© Multi-Phase Reasoning Pipeline

1. Planner  
   - Reads the question  
   - Produces a structured step-by-step plan  
   - Output is JSON-friendly and deterministic  

2. Executor  
   - Executes the plan exactly  
   - Performs calculations  
   - Returns intermediate_result and factual step records  
   - No chain-of-thought exposed  

3. Verifier  
   - Independently validates executor output  
   - Recomputes results  
   - Flags incorrect reasoning  
   - Triggers retries if needed  

4. Retry Controller  
   - Automatically retries up to a fixed limit  
   - Tracks retry count for evaluation  

---

## âœ… Expected JSON Output

Example response:
```
{
  "answer": 350,
  "status": "success",
  "reasoning_visible_to_user": "I solved this by planning, executing, and verifying the result.",
  "metadata": {
    "plan": {
      "steps": [
        "Calculate distance at 50 km/hr for 3 hours",
        "Calculate distance at 80 km/hr for 2 hours",
        "Sum both distances"
      ]
    },
    "checks": [
      {
        "check_name": "verification_passed",
        "passed": true,
        "details": ""
      }
    ],
    "retries": 0
  }
}
```
---

## ğŸ“ Project Structure
```
MultiStep_Reasoning/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ graph.py              # LangGraph definition
â”‚   â”œâ”€â”€ nodes.py              # Planner / Executor / Verifier nodes
â”‚   â””â”€â”€ graph_state.py        # State schema
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ planner_prompt.txt
â”‚   â”œâ”€â”€ executor_prompt.txt
â”‚   â””â”€â”€ verifier_prompt.txt
â”‚
â”œâ”€â”€ solve.py                  # Core solve() API
â”œâ”€â”€ app.py                    # Streamlit UI
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ easy_tests.py
â”‚   â”œâ”€â”€ tricky_tests.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py             # CSV / JSON logger
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ run_logs.csv
â”‚   
â”‚
â”œâ”€â”€ run_tests.py              # Test runner
â””â”€â”€ print_summary.py          # Test analytics
```
---

## ğŸ–¥ï¸ How to Run the Application

Start the Streamlit App:

```
streamlit run app.py
```
- Enter a question
- View final answer, explanation, retries, verifier checks, and raw JSON output

---

## ğŸ§ª Evaluation & Test Suite

The project includes a custom test framework to evaluate reasoning quality and robustness.

### Test Categories

Easy Tests:
- Basic arithmetic
- Speed Ã— time Ã— distance
- Simple time differences

Tricky Tests:
- Multi-step reasoning
- Ambiguous phrasing
- Time boundary cases
- Edge conditions

---

## â–¶ï¸ How to Run Tests

Run all tests:

python run_tests.py

Run individually:

```
python -m tests.easy_tests  
python -m tests.tricky_tests  
```
---

## ğŸ§ª What Each Test Logs

For every test case, the following are recorded:

- Question  
- Final JSON output  
- Whether the verifier passed  
- Number of retries performed  

---

## ğŸ“„ Logs Export

All runs (tests + app usage) are automatically logged to:

- logs/run_logs.csv  
 
---

## ğŸ” What the Logs Help Evaluate

- Planner accuracy  
- Executor consistency  
- Verifier effectiveness  
- Retry behavior  

---

## ğŸ“Š Auto-Generated Test Summary

Category | Test Count | Verifier Pass Rate | Retries Observed  
Easy     | 5â€“10       | ~100%              | 0â€“1  
Tricky  | 3â€“5        | ~85â€“90%            | 1â€“2  

---

## ğŸ“ˆ Retry & Quality Metrics

Using print_summary.py, the system computes:
- Retry rate
- Average retries per question
- Planner / Executor quality indicators
- Failure patterns

---

## âš ï¸ Rate-Limit Notes

- Running many tests consumes LLM tokens  
- For testing: use smaller models  
- For demo / production: use gpt-4o-mini  

---

## ğŸš§ Future Improvements

- Local math executor (no LLM for arithmetic)
- Caching planner outputs
- Parallel verifier strategies
- Streaming UI responses
- Confidence scoring

---

## âœ… Summary

- Clean architecture  
- Deterministic execution  
- Self-verification  
- Retry logic  
- Full evaluation framework  
- Production-ready logging  

