# ğŸ§  Multi-Step Reasoning Agent with Self-Checking  
A lightweight reasoning agent that solves structured word problems using **Planner â†’ Executor â†’ Verifier** architecture.  
It provides a clean JSON output with a user-facing answer and internal metadata (plan, checks, retries).  
Supports easy extension to any LLM provider (OpenAI, Anthropic, Gemini, etc.).

---

## ğŸš€ Features

### âœ… Multi-Phase Agent  
1. **Planner**  
   - Reads question  
   - Generates structured plan  
   - Example: parse â†’ extract values â†’ compute â†’ format  

2. **Executor**  
   - Executes plan  
   - Uses Python for calculations  
   - Runs LLM if required  
   - Stores intermediate results  

3. **Verifier**  
   - Independently verifies answer  
   - Recomputes / validates constraints  
   - Flags inconsistencies  
   - Supports retries  

## âœ… Clean JSON Output  
Example:
```json
{
  "answer": "3 hours 35 minutes",
  "status": "success",
  "reasoning_visible_to_user": "Calculated time difference between 14:30 and 18:05.",
  "metadata": {
    "plan": "...",
    "checks": [...],
    "retries": 0
  }
}
```

## ğŸ“ Project Structure

```
Mutli-Step-Reasoning-Agent-with-Self-Checking/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ graph.py
â”‚   â”œâ”€â”€ nodes.py
â”‚   â””â”€â”€ graph_state.py
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ planner_prompt.txt
â”‚   â”œâ”€â”€ executor_prompt.txt
â”‚   â””â”€â”€ verifier_prompt.txt
â”‚
â”œâ”€â”€ solve.py
â”‚
â””â”€â”€ tests/
      â”œâ”€â”€ test_easy.py
      â”œâ”€â”€ test_tricky.py
      â”œâ”€â”€ test_logs.json
      â”œâ”€â”€ test_logs.csv

```

## ğŸ§© Prompts

Prompts are stored in `prompts.py`:

## 1. Planner Prompt
- Generates numbered steps  
- Output must be JSON-friendly  

## 2. Executor Prompt
- Follows plan exactly  
- Returns intermediate calculations  

## 3. Verifier Prompt
- Re-computes result  
- Returns pass/fail + explanation  

Each prompt includes 2â€“3 few-shot examples.

---

# â–¶ï¸ How to Run

### Option A: CLI  
```
python agent.py  
```

### Option B: Use the function  
```
from solver import solve  
print(solve("Alice has 3 red apples and twice as many green apples. How many apples?"))
```
---

## ğŸ§ª Evaluation & Test Cases

The agent includes a small automated test suite to validate correctness,
robustness, and self-verification behavior.

### Test Categories

#### âœ… Easy Tests
- Basic arithmetic
- Speedâ€“timeâ€“distance
- Simple unit calculations

#### âš ï¸ Tricky Tests
- Multi-step reasoning
- Ambiguous phrasing
- Time boundary cases
- Edge cases (zero values)

### How to Run Tests

```bash
pytest tests/test_easy.py
pytest tests/test_tricky.py
```
## ğŸ§ª What Each Test Logs

For every test case, the following details are recorded:

- **Question** â€“ The original user query given to the agent  
- **Final JSON Output** â€“ The complete structured response produced by the agent  
- **Verifier Status** â€“ Whether the verifier approved the solution (`passed = true/false`)  
- **Retries Performed** â€“ Number of times the agent retried planning/execution  

---

## ğŸ“„ Logs Export

Test results are automatically exported to the following files:

- `tests/test_logs.json`
- `tests/test_logs.csv`

---

## ğŸ” What These Logs Help Evaluate

The exported logs are used to assess:

- **Planner Accuracy** â€“ Whether the agent correctly decomposes the problem  
- **Executor Consistency** â€“ Whether calculations follow the plan reliably  
- **Verifier Effectiveness** â€“ Whether incorrect or inconsistent answers are caught  
- **Retry Behavior** â€“ How often and when the agent self-corrects  

These artifacts make the agentâ€™s reasoning loop transparent and easy to evaluate during review.

---

## ğŸ“Š Test Summary

| Category | Test Count | Verifier Pass Rate | Retries Observed |
|--------|------------|--------------------|------------------|
| Easy   | 5          | 100%               | 0â€“1              |
| Tricky | 4          | ~90%               | 1â€“2              |

âœ” Easy tests validate deterministic reasoning  
âœ” Tricky tests stress multi-step planning and verification  
âœ” Retries confirm self-correction behavior


---

# ğŸ“š Prompt Design Notes

## What Worked
- Separating plan generation improves determinism  
- Executor runs cleanly with Python arithmetic  
- Verifier catches inconsistent LLM reasoning  

## What Didnâ€™t Work Initially
- Allowing executor to interpret the plan caused drift  
- Verifier needed strict JSON format to avoid false failures  

## Future Improvements
- Add caching to avoid repeated planner calls  
- Add streaming responses  
- Improve handling of ambiguous time formats  

---









